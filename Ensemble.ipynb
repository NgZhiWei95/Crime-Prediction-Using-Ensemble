{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzzlhOxksvX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63457d2-b707-45f6-8631-a6b44bc4a317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUYh9Dd9bzYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34aef58-91ff-4870-97eb-8926fbef1930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install function-pipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGr-Iz6zWkGA",
        "outputId": "4112c23f-b2d0-4f75-d5eb-69f5f42f6a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: function-pipe in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMjKhnhbxL2Y"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "import pyarrow as pa\n",
        "import function_pipe as fpn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from pyarrow import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from copy import deepcopy\n",
        "from abc import ABC, abstractmethod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjSaNwoWi40s"
      },
      "source": [
        "# Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7u9TbEUuWxS"
      },
      "outputs": [],
      "source": [
        "DATASET_FILENAME = '/content/gdrive/MyDrive/NYPD_Complaint_Data_Historic.csv'\n",
        "PALETTE = sns.color_palette('winter', as_cmap=True)\n",
        "THRESHOLD = 0\n",
        "UNUSED_LABELS = ['STATION_NAME', 'TRANSIT_DISTRICT', 'PARKS_NM'] # note: these labels are completely or mostly empty\n",
        "UNKNOWN_VALUES = ['UNKNOWN', 'U', '']\n",
        "LABELS_TO_BE_USED = [\n",
        "  'CMPLNT_NUM',\n",
        "  'BORO_NM',\n",
        "  'OFNS_DESC',\n",
        "  'CMPLNT_FR_DT',\n",
        "  'CMPLNT_FR_TM',\n",
        "  'CMPLNT_TO_DT',\n",
        "  'CMPLNT_TO_TM',\n",
        "  'HOUSING_PSA',\n",
        "  'LAW_CAT_CD',\n",
        "  'SUSP_AGE_GROUP',\n",
        "  'SUSP_RACE',\n",
        "  'SUSP_SEX',\n",
        "  'VIC_AGE_GROUP',\n",
        "  'VIC_RACE',\n",
        "  'VIC_SEX',\n",
        "  'Latitude',\n",
        "  'Longitude'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItiI-VKUeRQ5"
      },
      "outputs": [],
      "source": [
        "table = csv.read_csv(DATASET_FILENAME)\n",
        "raw_data = table.to_pandas(split_blocks=True, self_destruct=True)\n",
        "del table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhBX0q2AUcWP"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(raw_data):\n",
        "  def get_threshold(threshold):\n",
        "    return len(raw_data.columns)-len(UNUSED_LABELS)-threshold\n",
        "\n",
        "  def sort_unique(arr):\n",
        "    return np.sort(arr.unique())\n",
        "\n",
        "  # technically binary encoding\n",
        "  def to_numeric_encoding(feature):\n",
        "    return feature.replace({ \n",
        "      offence:i for (i, offence) in enumerate(sort_unique(feature)) \n",
        "    })\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_boro_nm(data):\n",
        "    # Maria's code (probably? not mine tho)\n",
        "    data['BORO_NM'] = data[\"BORO_NM\"].replace({ \"BROOKLYN\": 0,\"BRONX\" : 1, \"MANHATTAN\" : 2, \"QUEENS\": 3, \"STATEN ISLAND\" : 4})\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_ofns_desc(data):\n",
        "    # Maria's code (probably? not mine tho)\n",
        "    counts = data['OFNS_DESC'].value_counts()\n",
        "    data = data[~data['OFNS_DESC'].isin(counts[counts < 1000].index)]\n",
        "    # My code\n",
        "    CLASSES = sort_unique(data['OFNS_DESC'])\n",
        "    data['OFNS_DESC'] = to_numeric_encoding(data['OFNS_DESC'])\n",
        "    return CLASSES, data\n",
        "  \n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_datetime(data):\n",
        "    def time_to_numeric(time):\n",
        "      return time.hour * 60 + time.minute\n",
        "\n",
        "    time_arr_to_numeric = np.vectorize(time_to_numeric)\n",
        "\n",
        "    # Based on Maria's code (probably?)\n",
        "    data['CMPLNT_FR_DT'] = pd.to_datetime(data['CMPLNT_FR_DT'], format='%m/%d/%Y')\n",
        "    data['CMPLNT_FR_TM'] = time_arr_to_numeric(data['CMPLNT_FR_TM'])\n",
        "    data = data.drop( data[ data['CMPLNT_FR_DT'] < pd.Timestamp(2010,1,1) ].index)\n",
        "    data['CMPLNT_TO_DT'] = pd.to_datetime(data['CMPLNT_TO_DT'], format='%m/%d/%Y')\n",
        "    data['CMPLNT_TO_TM'] = pd.to_timedelta(data['CMPLNT_TO_TM'])\n",
        "    # My code\n",
        "    data.loc[:,'CMPLNT_FR_DT'] = pd.to_numeric(data.loc[:,'CMPLNT_FR_DT'])\n",
        "    data.loc[:,'CMPLNT_TO_DT'] = pd.to_numeric(data.loc[:,'CMPLNT_TO_DT'])\n",
        "    data.loc[:,'CMPLNT_FR_TM'] = pd.to_numeric(data.loc[:,'CMPLNT_FR_TM'])\n",
        "    data.loc[:,'CMPLNT_TO_TM'] = pd.to_numeric(data.loc[:,'CMPLNT_TO_TM'])\n",
        "    data.loc[:,'Duration'] = pd.to_numeric(data.loc[:,'CMPLNT_TO_DT'] - data.loc[:,'CMPLNT_FR_DT']) + (data.loc[:,'CMPLNT_TO_TM']  - data.loc[:,'CMPLNT_FR_TM'])\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_housing_psa(data):\n",
        "    # Maria's code (probably?)\n",
        "    # remove HOUSING_PSA with less than 10 \n",
        "    counts = data.loc[:,'HOUSING_PSA'].value_counts()\n",
        "    data.loc[:,:] = data[~data.loc[:,'HOUSING_PSA'].isin(counts[counts < 10].index)]\n",
        "    # mine\n",
        "    data['HOUSING_PSA'] = to_numeric_encoding(data['HOUSING_PSA'].astype(str))\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_law_cat_cd(data):\n",
        "    # Maria's code (probably?)\n",
        "    data.loc[:,'LAW_CAT_CD'] = data[\"LAW_CAT_CD\"].replace({ \"MISDEMEANOR\": 0, \"VIOLATION\" : 1, \"FELONY\" : 2})\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_sex(data):\n",
        "    # mine\n",
        "    data['VIC_SEX'] = pd.get_dummies(data[\"VIC_SEX\"])['M']\n",
        "    data[\"SUSP_SEX\"] = pd.get_dummies(data[\"SUSP_SEX\"])['M']\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_age_group(data):\n",
        "    # Maria's code (probably?)\n",
        "    data['SUSP_AGE_GROUP'] = data[\"SUSP_AGE_GROUP\"].replace({ \"<18\": 0,\"18-24\" : 1, \"25-44\" : 2, \"45-64\": 3, \"65+\" : 4})\n",
        "    data['VIC_AGE_GROUP'] = data[\"VIC_AGE_GROUP\"].replace({ \"<18\": 0,\"18-24\" : 1, \"25-44\" : 2, \"45-64\": 3, \"65+\" : 4})\n",
        "\n",
        "    data['SUSP_AGE_GROUP'] = pd.to_numeric(data['SUSP_AGE_GROUP'])\n",
        "    data['VIC_AGE_GROUP'] = pd.to_numeric(data['VIC_AGE_GROUP'])\n",
        "\n",
        "    data = data.drop(data[(data['SUSP_AGE_GROUP'] > 4) | (data['SUSP_AGE_GROUP'] < 0)].index)\n",
        "    data = data.drop(data[(data['VIC_AGE_GROUP'] > 4) | (data['VIC_AGE_GROUP'] < 0)].index)\n",
        "    return data\n",
        "\n",
        "  @fpn.FunctionNode\n",
        "  def preprocess_race(data):\n",
        "    def one_hot_race(feature):\n",
        "      oh_feature = pd.get_dummies(feature)\n",
        "      oh_feature['HISPANIC'] = oh_feature['BLACK HISPANIC'] | oh_feature['WHITE HISPANIC']\n",
        "      oh_feature['BLACK'] |= oh_feature['BLACK HISPANIC']\n",
        "      oh_feature['WHITE'] |= oh_feature['WHITE HISPANIC']\n",
        "      oh_feature.drop(columns=['BLACK HISPANIC','WHITE HISPANIC'], axis=1, inplace=True)\n",
        "      return oh_feature\n",
        "\n",
        "    oh_vic_race = one_hot_race(data['VIC_RACE'])\n",
        "    oh_susp_race = one_hot_race(data['SUSP_RACE'])\n",
        "\n",
        "    for race in oh_vic_race:\n",
        "      data['VIC_' + race] = oh_vic_race[race]\n",
        "\n",
        "    for race in oh_susp_race:\n",
        "      data['SUSP_' + race] = oh_susp_race[race]\n",
        "\n",
        "    data.drop(columns=['VIC_RACE','SUSP_RACE'], axis=1, inplace=True)\n",
        "    return data\n",
        "\n",
        "  data = raw_data \\\n",
        "  .drop(columns=UNUSED_LABELS) \\\n",
        "  .replace({ val:np.nan for val in UNKNOWN_VALUES }) \\\n",
        "  .dropna(thresh=get_threshold(0)) \\\n",
        "  .loc[:, LABELS_TO_BE_USED]\n",
        "  \n",
        "  CLASSES, data = (preprocess_boro_nm >> preprocess_ofns_desc)(data)\n",
        "\n",
        "  return [\n",
        "    CLASSES,\n",
        "    (\n",
        "        preprocess_datetime >>\n",
        "        preprocess_housing_psa >>\n",
        "        preprocess_law_cat_cd >>\n",
        "        preprocess_sex >>\n",
        "        preprocess_age_group >>\n",
        "        preprocess_race\n",
        "     \n",
        "    )(data).dropna()\n",
        "  ]\n",
        "\n",
        "CLASSES, data = preprocess_data(raw_data)\n"
      ],
      "metadata": {
        "id": "Lf8GSUzIFmTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372e996b-499b-416f-d22f-2dd4271267a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "mRtfOVJGlN1Y",
        "outputId": "2fd6a9bd-1433-4b4b-d11c-d6e8901c93ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          CMPLNT_NUM  BORO_NM  OFNS_DESC  CMPLNT_FR_DT  CMPLNT_FR_TM  \\\n",
              "33       593134992.0      0.0        2.0  1.374278e+18        1110.0   \n",
              "168      563768013.0      0.0        0.0  1.497485e+18         210.0   \n",
              "178      677977125.0      1.0        0.0  1.480550e+18         181.0   \n",
              "247      380439115.0      0.0        0.0  1.456013e+18        1225.0   \n",
              "266      991203856.0      2.0        5.0  1.404691e+18        1063.0   \n",
              "...              ...      ...        ...           ...           ...   \n",
              "6982942  368358311.0      0.0        3.0  1.512432e+18         660.0   \n",
              "6983012  622273511.0      2.0        3.0  1.520726e+18        1230.0   \n",
              "6983079  808144426.0      2.0        0.0  1.517011e+18         219.0   \n",
              "6983092  255356017.0      0.0        8.0  1.543450e+18        1355.0   \n",
              "6983157  488162445.0      2.0        3.0  1.526688e+18        1050.0   \n",
              "\n",
              "         CMPLNT_TO_DT  CMPLNT_TO_TM  HOUSING_PSA  LAW_CAT_CD  SUSP_AGE_GROUP  \\\n",
              "33       1.374278e+18  6.672000e+13           49         2.0             2.0   \n",
              "168      1.497485e+18  2.250000e+13           57         0.0             2.0   \n",
              "178      1.480550e+18  1.482000e+13          404         0.0             2.0   \n",
              "247      1.456013e+18  7.380000e+13          252         0.0             2.0   \n",
              "266      1.404691e+18  6.480000e+13          310         0.0             2.0   \n",
              "...               ...           ...          ...         ...             ...   \n",
              "6982942  1.512432e+18  4.050000e+13          208         1.0             3.0   \n",
              "6983012  1.520726e+18  7.560000e+13          351         1.0             3.0   \n",
              "6983079  1.517011e+18  1.404000e+13          318         0.0             2.0   \n",
              "6983092  1.543450e+18  8.160000e+13          222         2.0             2.0   \n",
              "6983157  1.526688e+18  6.330000e+13          342         1.0             3.0   \n",
              "\n",
              "         ...  VIC_AMERICAN INDIAN/ALASKAN NATIVE  \\\n",
              "33       ...                                   0   \n",
              "168      ...                                   0   \n",
              "178      ...                                   0   \n",
              "247      ...                                   0   \n",
              "266      ...                                   0   \n",
              "...      ...                                 ...   \n",
              "6982942  ...                                   0   \n",
              "6983012  ...                                   0   \n",
              "6983079  ...                                   0   \n",
              "6983092  ...                                   0   \n",
              "6983157  ...                                   0   \n",
              "\n",
              "         VIC_ASIAN / PACIFIC ISLANDER  VIC_BLACK  VIC_WHITE  VIC_HISPANIC  \\\n",
              "33                                  0          1          0             0   \n",
              "168                                 0          1          0             0   \n",
              "178                                 0          0          1             1   \n",
              "247                                 0          1          0             0   \n",
              "266                                 0          0          1             1   \n",
              "...                               ...        ...        ...           ...   \n",
              "6982942                             0          1          0             0   \n",
              "6983012                             0          1          0             1   \n",
              "6983079                             0          1          0             1   \n",
              "6983092                             0          0          1             0   \n",
              "6983157                             0          0          1             1   \n",
              "\n",
              "         SUSP_AMERICAN INDIAN/ALASKAN NATIVE  SUSP_ASIAN / PACIFIC ISLANDER  \\\n",
              "33                                         0                              0   \n",
              "168                                        0                              0   \n",
              "178                                        0                              0   \n",
              "247                                        0                              0   \n",
              "266                                        0                              0   \n",
              "...                                      ...                            ...   \n",
              "6982942                                    0                              0   \n",
              "6983012                                    0                              0   \n",
              "6983079                                    0                              0   \n",
              "6983092                                    0                              0   \n",
              "6983157                                    0                              0   \n",
              "\n",
              "         SUSP_BLACK  SUSP_WHITE  SUSP_HISPANIC  \n",
              "33                1           0              0  \n",
              "168               1           0              0  \n",
              "178               0           1              1  \n",
              "247               1           0              0  \n",
              "266               1           0              0  \n",
              "...             ...         ...            ...  \n",
              "6982942           1           0              0  \n",
              "6983012           0           1              1  \n",
              "6983079           0           1              1  \n",
              "6983092           1           0              0  \n",
              "6983157           1           0              0  \n",
              "\n",
              "[55644 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef0e5dd2-efaa-448e-844b-357ff86761b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CMPLNT_NUM</th>\n",
              "      <th>BORO_NM</th>\n",
              "      <th>OFNS_DESC</th>\n",
              "      <th>CMPLNT_FR_DT</th>\n",
              "      <th>CMPLNT_FR_TM</th>\n",
              "      <th>CMPLNT_TO_DT</th>\n",
              "      <th>CMPLNT_TO_TM</th>\n",
              "      <th>HOUSING_PSA</th>\n",
              "      <th>LAW_CAT_CD</th>\n",
              "      <th>SUSP_AGE_GROUP</th>\n",
              "      <th>...</th>\n",
              "      <th>VIC_AMERICAN INDIAN/ALASKAN NATIVE</th>\n",
              "      <th>VIC_ASIAN / PACIFIC ISLANDER</th>\n",
              "      <th>VIC_BLACK</th>\n",
              "      <th>VIC_WHITE</th>\n",
              "      <th>VIC_HISPANIC</th>\n",
              "      <th>SUSP_AMERICAN INDIAN/ALASKAN NATIVE</th>\n",
              "      <th>SUSP_ASIAN / PACIFIC ISLANDER</th>\n",
              "      <th>SUSP_BLACK</th>\n",
              "      <th>SUSP_WHITE</th>\n",
              "      <th>SUSP_HISPANIC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>593134992.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.374278e+18</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>1.374278e+18</td>\n",
              "      <td>6.672000e+13</td>\n",
              "      <td>49</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>563768013.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.497485e+18</td>\n",
              "      <td>210.0</td>\n",
              "      <td>1.497485e+18</td>\n",
              "      <td>2.250000e+13</td>\n",
              "      <td>57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>677977125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.480550e+18</td>\n",
              "      <td>181.0</td>\n",
              "      <td>1.480550e+18</td>\n",
              "      <td>1.482000e+13</td>\n",
              "      <td>404</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>380439115.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.456013e+18</td>\n",
              "      <td>1225.0</td>\n",
              "      <td>1.456013e+18</td>\n",
              "      <td>7.380000e+13</td>\n",
              "      <td>252</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>991203856.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.404691e+18</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>1.404691e+18</td>\n",
              "      <td>6.480000e+13</td>\n",
              "      <td>310</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6982942</th>\n",
              "      <td>368358311.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.512432e+18</td>\n",
              "      <td>660.0</td>\n",
              "      <td>1.512432e+18</td>\n",
              "      <td>4.050000e+13</td>\n",
              "      <td>208</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6983012</th>\n",
              "      <td>622273511.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.520726e+18</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>1.520726e+18</td>\n",
              "      <td>7.560000e+13</td>\n",
              "      <td>351</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6983079</th>\n",
              "      <td>808144426.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.517011e+18</td>\n",
              "      <td>219.0</td>\n",
              "      <td>1.517011e+18</td>\n",
              "      <td>1.404000e+13</td>\n",
              "      <td>318</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6983092</th>\n",
              "      <td>255356017.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.543450e+18</td>\n",
              "      <td>1355.0</td>\n",
              "      <td>1.543450e+18</td>\n",
              "      <td>8.160000e+13</td>\n",
              "      <td>222</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6983157</th>\n",
              "      <td>488162445.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.526688e+18</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>1.526688e+18</td>\n",
              "      <td>6.330000e+13</td>\n",
              "      <td>342</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55644 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef0e5dd2-efaa-448e-844b-357ff86761b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef0e5dd2-efaa-448e-844b-357ff86761b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef0e5dd2-efaa-448e-844b-357ff86761b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4QWV9RBlGm4",
        "outputId": "4a3e9ad0-8409-47ce-9e04-aab981ac55e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CMPLNT_NUM                             float64\n",
              "BORO_NM                                float64\n",
              "OFNS_DESC                              float64\n",
              "CMPLNT_FR_DT                           float64\n",
              "CMPLNT_FR_TM                           float64\n",
              "CMPLNT_TO_DT                           float64\n",
              "CMPLNT_TO_TM                           float64\n",
              "HOUSING_PSA                              int64\n",
              "LAW_CAT_CD                             float64\n",
              "SUSP_AGE_GROUP                         float64\n",
              "SUSP_SEX                                 uint8\n",
              "VIC_AGE_GROUP                          float64\n",
              "VIC_SEX                                  uint8\n",
              "Latitude                               float64\n",
              "Longitude                              float64\n",
              "Duration                               float64\n",
              "VIC_AMERICAN INDIAN/ALASKAN NATIVE       uint8\n",
              "VIC_ASIAN / PACIFIC ISLANDER             uint8\n",
              "VIC_BLACK                                uint8\n",
              "VIC_WHITE                                uint8\n",
              "VIC_HISPANIC                             uint8\n",
              "SUSP_AMERICAN INDIAN/ALASKAN NATIVE      uint8\n",
              "SUSP_ASIAN / PACIFIC ISLANDER            uint8\n",
              "SUSP_BLACK                               uint8\n",
              "SUSP_WHITE                               uint8\n",
              "SUSP_HISPANIC                            uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR1iiLBrrBvE"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split training set"
      ],
      "metadata": {
        "id": "QHpW1ZMBr7DI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blEsXG0_xtcx"
      },
      "outputs": [],
      "source": [
        "# Based on https://stackoverflow.com/questions/68279596/split-pandas-dataframe-by-label-with-ratio\n",
        "# Splitting set into training set and test set\n",
        "def split_training_set(data):\n",
        "  training_set = data.sample(frac=0.80, random_state = 0) \n",
        "  test_set = data.drop(training_set.index)\n",
        "  train_x, train_y = training_set.drop(columns=['OFNS_DESC']), training_set.loc[:, 'OFNS_DESC']\n",
        "  test_x, test_y = test_set.drop(columns=['OFNS_DESC']), test_set.loc[:, 'OFNS_DESC']\n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n",
        "train_x, train_y, test_x, test_y = split_training_set(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyAeUTzElhu0",
        "outputId": "415ca8dd-2e98-49ff-f6b3-ac31ab6d143c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CMPLNT_NUM                             float64\n",
              "BORO_NM                                float64\n",
              "CMPLNT_FR_DT                           float64\n",
              "CMPLNT_FR_TM                           float64\n",
              "CMPLNT_TO_DT                           float64\n",
              "CMPLNT_TO_TM                           float64\n",
              "HOUSING_PSA                              int64\n",
              "LAW_CAT_CD                             float64\n",
              "SUSP_AGE_GROUP                         float64\n",
              "SUSP_SEX                                 uint8\n",
              "VIC_AGE_GROUP                          float64\n",
              "VIC_SEX                                  uint8\n",
              "Latitude                               float64\n",
              "Longitude                              float64\n",
              "Duration                               float64\n",
              "VIC_AMERICAN INDIAN/ALASKAN NATIVE       uint8\n",
              "VIC_ASIAN / PACIFIC ISLANDER             uint8\n",
              "VIC_BLACK                                uint8\n",
              "VIC_WHITE                                uint8\n",
              "VIC_HISPANIC                             uint8\n",
              "SUSP_AMERICAN INDIAN/ALASKAN NATIVE      uint8\n",
              "SUSP_ASIAN / PACIFIC ISLANDER            uint8\n",
              "SUSP_BLACK                               uint8\n",
              "SUSP_WHITE                               uint8\n",
              "SUSP_HISPANIC                            uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boost"
      ],
      "metadata": {
        "id": "H1ekL9_OsEf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting Classifier Class\n",
        "This class is a base or abstract class for all boosting methods. It contains the fit, predict, and predict_proba methods, which can also be found on the boosting methods from SciKit."
      ],
      "metadata": {
        "id": "BaPXXjdUsHsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoostingClassifier(ABC):\n",
        "  def __init__(\n",
        "    self,\n",
        "    n_iterations,\n",
        "    weak_learner,\n",
        "    loss\n",
        "  ):\n",
        "    self.n_iterations = n_iterations\n",
        "    self.weak_learner = weak_learner\n",
        "    self.loss = loss\n",
        "    self.models = np.full(n_iterations, None)\n",
        "    self.accuracy = None\n",
        "\n",
        "  @abstractmethod\n",
        "  def fit(self, dataset_x: np.ndarray, dataset_y: np.ndarray):\n",
        "    pass\n",
        "  \n",
        "  @abstractmethod\n",
        "  def predict(self, dataset_x: np.ndarray, dataset_y: np.ndarray):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def predict_proba(self, dataset_x: np.ndarray, dataset_y: np.ndarray):\n",
        "    pass\n",
        "\n",
        "  def calculate_accuracy(self, dataset_x: np.ndarray, dataset_y: np.ndarray):\n",
        "    return 100*sum(self.predict(dataset_x) == dataset_y)/dataset_y.size\n"
      ],
      "metadata": {
        "id": "oQZQnZcHCY9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaboost\n",
        "\n",
        "This class is an implementation of AdaBoost, which is a subclass of BoostingClassifier. The `fit` method and `predict` method are based on the algorithms/pseudocode found on [this lecture](https://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/AdaBoost.pdf).\n",
        "\n",
        "#### Loss function\n",
        "Uses ~~`np.int64`~~ `np.int8` (note: I just changed it to `np.int8`. I have not tested it yet, so I have no idea if it has side effects or not.) to convert `True` to `1` and `False` to `0`. The conditional that is passed to this loss function ($I$) is `f_iteration.predict(dataset_x[i].reshape(1,-1)) != dataset_y[i]` or $f_m(x_i) \\neq y_i$. As seen on the lecture, if $f_m(x_i) \\neq y_i$ is true, the output of $I$ is $1$. If not, the output of $I$ is $0$.\n",
        "\n",
        "In the lecture I\n",
        "\n",
        "#### Fit method\n",
        "Based on the \"AdaBoost algorithm\" found on the lecture.\n",
        "\n",
        "This method uses `deepcopy` since Python is a bit finnicky when it comes to assigning variables. It's a small Band-Aid I used to fix a bug that might take a long time to fix.\n",
        "\n",
        "#### Predict method\n",
        "In SciKit, the `predict` method for AdaBoost is the \"weighted mean prediction\" ([source](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier.predict)). This method technically does not exist, but since BoostingClassifier requires a `predict` method, The later secion is used as the predict method.\n",
        "\n",
        "##### Binary Predict method\n",
        "This is the method that is used by MultiBoost classifier. This is based on the `predict` algorithm or rather $g(x)$ of the algorithm on the lecture.\n",
        "As mentioned, the algorithm from the paper is $g(x) = \\text{sign}(\\sum_{m=1}^{M}\\alpha_mf_m(x))$ .\n",
        "\n",
        "\n",
        "#### Predict_proba method\n",
        "As of now, this method does nothing, but it will be a very important method to MultiBoost once completed (check later section).\n",
        "\n"
      ],
      "metadata": {
        "id": "gnaV_80usaw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk7b-m4nhVnQ"
      },
      "outputs": [],
      "source": [
        "# resources:\n",
        "# https://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/AdaBoost.pdf\n",
        "class AdaBoost(BoostingClassifier):\n",
        "  def __init__(self, n_iterations, weak_learner):\n",
        "    super().__init__(\n",
        "        n_iterations=n_iterations,\n",
        "        loss=self.loss,\n",
        "        weak_learner=weak_learner\n",
        "    )\n",
        "\n",
        "  def loss(self, conditional):\n",
        "    return np.int64(conditional)\n",
        "\n",
        "  def fit(self, dataset_x, dataset_y):\n",
        "    # Initialize weight distribution vector\n",
        "    # Note: N = dataset size, M =  num_of_classifiers/iteartions\n",
        "    dataset_x = np.float64(dataset_x)\n",
        "    dataset_y = np.float64(dataset_y)\n",
        "    dataset_size = dataset_y.size\n",
        "    weight_distribution_vector = np.ones(dataset_size) * (1/dataset_size)  \n",
        "    self.models = np.full(self.n_iterations, None)\n",
        "    for iteration in range(1, self.n_iterations+1): # for m = 1 to M do\n",
        "      f_iteration = deepcopy(self.weak_learner).fit(dataset_x, dataset_y, sample_weight=weight_distribution_vector)\n",
        "      Ɛ_iteration = sum(\n",
        "          [\n",
        "            weight_distribution_vector[i] * \\\n",
        "            self.loss(f_iteration.predict(dataset_x[i].reshape(1,-1)) != dataset_y[i]) \\\n",
        "            for i in range(dataset_size)\n",
        "          ]\n",
        "      ) / weight_distribution_vector.sum()\n",
        "      α_iteration = np.log((1- Ɛ_iteration)/Ɛ_iteration)\n",
        "      for i in range(dataset_size):\n",
        "        weight_distribution_vector[i] = \\\n",
        "          weight_distribution_vector[i] * \\\n",
        "          np.exp(\n",
        "            α_iteration * self.loss(f_iteration.predict(dataset_x[i].reshape(1,-1)) != dataset_y[i])\n",
        "          )\n",
        "      self.models[iteration-1] = (α_iteration, f_iteration)\n",
        "    return self\n",
        "\n",
        "  def predict_proba(self, dataset_x):\n",
        "    return self.models[-1][-1].predict_proba(np.float64(dataset_x))\n",
        "\n",
        "  #  g(x) = np.sign(sum(m=1, M, alpha_m * f_m(x)))\n",
        "  def predict(self, d_x):\n",
        "    self.binary_predict(self, d_x)\n",
        "\n",
        "  # g(x) = np.sign(sum(m=1, M, alpha_m * f_m(x)))\n",
        "  def binary_predict(self, d_x):\n",
        "    def calculate_output(x):\n",
        "      return lambda output: output[0] * output[1].predict(x)\n",
        "\n",
        "    pred_y = np.zeros((d_x.shape[0]))  \n",
        "    dataset_x = np.float64(d_x)\n",
        "\n",
        "    for model in self.models:\n",
        "      pred_y += calculate_output(dataset_x)(model)\n",
        "\n",
        "    return np.sign(pred_y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ys = pd.get_dummies(train_y).astype('int8').replace(0, -1)\n",
        "\n",
        "a = AdaBoost(50, DecisionTreeClassifier(max_depth=2))\n",
        "a.fit(train_x, dataset_ys[3.0])\n",
        "a.binary_predict(train_x)\n",
        "\n",
        "sum(a.binary_predict(train_x) == dataset_ys[5.0])*100/train_y.size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sJH8LIJjvQQ",
        "outputId": "f0a42a80-0e2e-48d0-ba24-44b3b8994cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61.29843872851848"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiBoost Classifier\n",
        "\n",
        "I'm not sure how original this is haha since I came up with this on the spot, but anyway, this is a \"meta\" classifier that is made up of other classifiers, which in this case is a boosting classifier, but it can be used with other meta-classifiers as well like bagging I think?\n",
        "\n",
        "(Note: Target dataset = dataset_y, not sure what's the correct term to use)\n",
        "\n",
        "This classifier uses **One-Hot Encoding** to turn a multi-class target dataset (`dataset_y`) into a target dataset made up of several binary-class features. This is useful for boosting algorithms that require a binary-class target dataset like `LPBoost`, or in this case `AdaBoost` (note: AdaBoost can accept multi-class target datasets, but I have no idea how to implement the `predict` method for multi-class target datasets, so...). \n",
        "\n",
        "I want to use LPBoost, but I didn't have time. Just mention it on the *future things to do*/*Future Improvements* part that I planned on using **LPBoost** as it might be a better fit than **AdaBoost** for this.\n",
        "\n",
        "Also, I already mentioned this, but this classifier works for other classifiers, not just AdaBoost.\n",
        "\n",
        "#### Fit method\n",
        "This method splits `dataset_y` into multiple binary `y datasets` and creates a AdaBoost model and fits it for every binary `y dataset`.\n",
        "\n",
        "This method uses `deepcopy` since Python is a bit finnicky when it comes to assigning variables. It's a small Band-Aid I used to fix a bug that might take a long time to fix.\n",
        "\n",
        "#### Merge_y method\n",
        "This method is important, but as of now, for every $x$th row, it removes all the columns that contain `NaN` for the $x$ row, and chooses the maximum. It shouldn't use the maximum, but since I don't have time, that's what I used. What this method should actually do is to choose the class value (note: class value = y/target) that has the highest probability of being correct using the `predict_proba` method. I haven't implemented it to work that way though, but that's how the code should work just in case the row has more than one non-`NaN` value. You can add this part to **Future Improvements** as well.\n",
        "\n",
        "#### Predict method\n",
        "This method predicts the value by going through every AdaBoost model that would predict `dataset_x`. The output of every model is either $1$ or $-1$. This method converts that to the class value and `NaN` respectively (i.e. when the output is `1`, it turns into the class value. When the output is `-1` it turns into `NaN`). It then uses `merge_y` method to merge all the predicted $y$ values into one $y$ dataset.\n",
        "It then returns the merged $y$ dataset."
      ],
      "metadata": {
        "id": "Wf4Mj0F6sNdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiBoostClassifier():\n",
        "  def __init__(self, boost_model, boost_params):\n",
        "    self.models = {}\n",
        "    self.boost_model = boost_model\n",
        "    self.boost_params = boost_params\n",
        "\n",
        "  def fit(self, dataset_x, dataset_y):\n",
        "    dataset_ys = pd.get_dummies(dataset_y).astype('int8').replace(0, -1)\n",
        "    for class_val in dataset_ys:\n",
        "      self.models[class_val] = deepcopy(self.boost_model(*self.boost_params)).fit(dataset_x, dataset_ys[class_val])\n",
        "\n",
        "\n",
        "  def predict(self,dataset_x):\n",
        "    # TODO, but no time: Instead of first, use the highest probability\n",
        "    def merge_y(x):\n",
        "      return x.dropna().astype(np.int8).max()\n",
        "\n",
        "    result = {}\n",
        "    for class_val in self.models:\n",
        "      map_to_class_val = lambda x : class_val if x == 1 else np.nan\n",
        "      map_to_class_val_vec = np.vectorize(map_to_class_val)\n",
        "      result[class_val] = map_to_class_val_vec(self.models[class_val].binary_predict(dataset_x))    \n",
        "\n",
        "    df_ys = pd.DataFrame(result)\n",
        "    cols = df_ys.columns \n",
        "    df_ys['y'] = df_ys[cols].apply(merge_y, 1)\n",
        "    return np.int8(df_ys.drop(cols, axis=1)['y'])\n",
        "\n",
        "  def calculate_accuracy(self, actual_y: np.ndarray, predicted_y: np.ndarray):\n",
        "    return 100*sum(actual_y == predicted_y)/predicted_y.size\n",
        "    # return 100*sum(self.predict(dataset_x) == dataset_y)/dataset_y.size\n"
      ],
      "metadata": {
        "id": "leJ1fER53tb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiboost = MultiBoostClassifier(\n",
        "    AdaBoost,\n",
        "    [150, DecisionTreeClassifier(max_depth=4)]\n",
        ")\n",
        "multiboost.fit(train_x, train_y)\n",
        "sum(multiboost.predict(train_x) == train_y)*100/train_y.size"
      ],
      "metadata": {
        "id": "AjdPPSWKDZys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiboost = MultiBoostClassifier(\n",
        "#     AdaBoost,\n",
        "#     [50, DecisionTreeClassifier(max_depth=4)]\n",
        "# )\n",
        "# multiboost.fit(train_x, train_y)\n",
        "# sum(multiboost.predict(train_x) == train_y)*100/train_y.size"
      ],
      "metadata": {
        "id": "ByZDVM0RttYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(multiboost.predict(train_x) == train_y)*100/train_y.size"
      ],
      "metadata": {
        "id": "cqO8sUmT08xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "UcsI2t9VR00E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
        "\n",
        "PALETTE = sns.color_palette(\"crest\", as_cmap=True)\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "def print_cm(actual_y, pred_y, title):\n",
        "  heatmap_data = pd.crosstab(actual_y, pred_y, normalize=\"index\").rename_axis(columns='Predicted Values', index='Actual Values')\n",
        "  annot = pd.crosstab(actual_y, pred_y)\n",
        "  _, ax = plt.subplots(figsize=(15,15))\n",
        "  ax = sns.heatmap(\n",
        "      heatmap_data,\n",
        "      annot=annot,\n",
        "      square=True,\n",
        "      fmt='.2f',\n",
        "      cmap=PALETTE,\n",
        "      cbar_kws= \n",
        "        dict(\n",
        "          use_gridspec=False,\n",
        "          orientation='horizontal',\n",
        "          shrink=0.7,\n",
        "          pad=0.05\n",
        "        ) # based on https://stackoverflow.com/a/47916308\n",
        "      )\n",
        "\n",
        "  acc_text = {\n",
        "    'Accuracy': 100*sum(actual_y == pred_y)/pred_y.size,\n",
        "    'F1-score (macro)': f1_score(actual_y, pred_y, average='macro'),\n",
        "    'F1-score (micro)': f1_score(actual_y, pred_y, average='micro'),\n",
        "    'F1-score (weighted)': f1_score(actual_y, pred_y, average='weighted'),\n",
        "    'Precision (macro)': precision_score(actual_y, pred_y, average='macro'),\n",
        "    'Precision (micro)': precision_score(actual_y, pred_y, average='micro'),\n",
        "    'Precision (weighted)': precision_score(actual_y, pred_y, average='weighted'),\n",
        "    'Recall (macro)': recall_score(actual_y, pred_y, average='macro'),\n",
        "    'Recall (micro)': recall_score(actual_y, pred_y, average='micro'),\n",
        "    'Recall (weighted)': recall_score(actual_y, pred_y, average='weighted'),\n",
        "    'AUC Score (OvR, macro)': roc_auc_score(actual_y, pd.get_dummies(pred_y), average='macro', multi_class='ovr'),\n",
        "    'AUC Score (OvO, macro)': roc_auc_score(actual_y, pd.get_dummies(pred_y), average='macro', multi_class='ovo'),\n",
        "    'AUC Score (OvR, weighted)': roc_auc_score(actual_y, pd.get_dummies(pred_y), average='weighted', multi_class='ovr'),\n",
        "    'AUC Score (OvO, weighted)': roc_auc_score(actual_y, pd.get_dummies(pred_y), average='weighted', multi_class='ovo')\n",
        "  }\n",
        "\n",
        "  text_arr = [f'{k}: {v}' for k, v in acc_text.items()] #] + [f'F1-score (class {i}): {val}' for i, val in enumerate(f1_score(actual_y, pred_y, average=None))]\n",
        "\n",
        "  # based on https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n",
        "  ax.text(\n",
        "      1/4,\n",
        "      -0.25,\n",
        "      '\\n'.join(text_arr[:len(text_arr)//2]),\n",
        "      horizontalalignment='center',\n",
        "      verticalalignment='center',\n",
        "      transform=ax.transAxes    \n",
        "  ) \n",
        "\n",
        "  # based on https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n",
        "  ax.text(\n",
        "      3/4,\n",
        "      -0.25,\n",
        "      '\\n'.join(text_arr[len(text_arr)//2:]),\n",
        "      horizontalalignment='center',\n",
        "      verticalalignment='center',\n",
        "      transform=ax.transAxes    \n",
        "  )\n",
        "\n",
        "  ax.set_title(title)\n",
        "\n",
        "#  ax.set_xlabel('123')"
      ],
      "metadata": {
        "id": "mBbF9J9RV36Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_cm(train_y, multiboost.predict(train_x), 'Training Set')"
      ],
      "metadata": {
        "id": "6Ir1b8qI2ldo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_cm(test_y, multiboost.predict(test_x), 'Test Set')"
      ],
      "metadata": {
        "id": "0n18IYRB2s6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#plot-roc-curves-for-the-multiclass-problem\n",
        "def get_roc_curve(actual_y, pred_y):  \n",
        "  true_y = actual_y if isinstance(actual_y, pd.DataFrame) else pd.get_dummies(actual_y)\n",
        "  test_y = pred_y if isinstance(pred_y, pd.DataFrame) else pd.get_dummies(pred_y)\n",
        "  classes = np.sort(true_y.columns.unique())\n",
        "\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "\n",
        "  for class_val in classes:\n",
        "    fpr[class_val], tpr[class_val], _ = roc_curve(true_y.loc[:, class_val], test_y.loc[:, class_val])\n",
        "    roc_auc[class_val] = auc(fpr[class_val], tpr[class_val])\n",
        "\n",
        "  fpr['Micro-average'], tpr['Micro-average'], _ = roc_curve(true_y.values.ravel(), test_y.values.ravel())\n",
        "  roc_auc['Micro-average'] = auc(fpr['Micro-average'], tpr['Micro-average'])\n",
        " \n",
        "  all_fpr = np.unique(\n",
        "      np.concatenate(\n",
        "        [fpr[class_val] for class_val in classes]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  mean_tpr = np.zeros_like(all_fpr)\n",
        "\n",
        "  for class_val in classes:\n",
        "    mean_tpr += np.interp(all_fpr, fpr[class_val], tpr[class_val])\n",
        "\n",
        "  # print(all_fpr)\n",
        "  # print(fpr)\n",
        "  # print(tpr)\n",
        "  # print(fpr[0].dtype)\n",
        "\n",
        "  mean_tpr /= classes.size\n",
        "\n",
        "  fpr['Macro-average'] = all_fpr\n",
        "  tpr['Macro-average'] = mean_tpr\n",
        "  roc_auc['Macro-average'] = auc(fpr['Macro-average'], tpr['Macro-average'])\n",
        "\n",
        "  df = pd.concat(\n",
        "    [\n",
        "      pd.DataFrame({\n",
        "        'False positive rate': fpr[class_val], \n",
        "        'True positive rate': tpr[class_val], \n",
        "        # 'type': f'ROC curve of {class_val if isinstance(class_val, str) else CLASSES[int(class_val)]} (area: {roc_auc[class_val].toFixed(3)})'\n",
        "        'type': f'{class_val if isinstance(class_val, str) else CLASSES[int(class_val)].title()} (area: {np.format_float_positional(roc_auc[class_val], precision=3, min_digits=3)})'\n",
        "      }) for class_val in fpr\n",
        "    ]\n",
        "  ).reset_index(drop=True)\n",
        "\n",
        "  a = sns.relplot(\n",
        "      data=df,\n",
        "      x='False positive rate',\n",
        "      y='True positive rate',\n",
        "      kind='line',\n",
        "      hue='type',\n",
        "      style='type',\n",
        "      palette=sns.color_palette(\n",
        "          palette='Pastel2',\n",
        "          n_colors=df['type'].unique().size\n",
        "      ),\n",
        "      aspect=1.2\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "get_roc_curve(test_y, multiboost.predict(test_x))\n"
      ],
      "metadata": {
        "id": "DXXrd-Fx3gYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: still working on this one, it's the same as above\n",
        "# But rewritten\n",
        "from scoping import scoping\n",
        "from pandas.core.dtypes.missing import array_equals\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#plot-roc-curves-for-the-multiclass-problem\n",
        "def get_roc_curve(actual_y, pred_y):  \n",
        "  true_y = actual_y if isinstance(actual_y, pd.DataFrame) else pd.get_dummies(actual_y)\n",
        "  test_y = pred_y if isinstance(pred_y, pd.DataFrame) else pd.get_dummies(pred_y)\n",
        "  classes = np.sort(true_y.columns.unique())\n",
        "\n",
        "  def to_class_name(name, fpr, tpr):\n",
        "    return f'{name} (area: {fix_float(auc(fpr, tpr), 3, 3)})'\n",
        "\n",
        "  def fix_float(val, precision, min_digits):\n",
        "    return np.format_float_positional(\n",
        "      val,\n",
        "      precision=precision,\n",
        "      min_digits=min_digits\n",
        "    )\n",
        "\n",
        "  def get_roc_for_class(class_val):\n",
        "    fpr, tpr, _ = roc_curve(true_y.loc[:, class_val], test_y.loc[:, class_val])\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    return pd.DataFrame({\n",
        "      'False positive rate': fpr.astype('float64'), \n",
        "      'True positive rate': tpr.astype('float64'), \n",
        "      'Class': to_class_name(CLASSES[int(class_val)].title(), fpr, tpr)\n",
        "    })\n",
        "\n",
        "  df = pd.concat(\n",
        "    [get_roc_for_class(class_val) for class_val in classes]  \n",
        "  ).reset_index(drop=True)\n",
        " \n",
        "  all_fpr = np.unique(df['False positive rate'])\n",
        "\n",
        "  with scoping():\n",
        "    fpr, tpr, _ = roc_curve(true_y.values.ravel(), test_y.values.ravel())\n",
        "    df = pd.concat([df, pd.DataFrame({\n",
        "        'False positive rate': fpr.astype('float64'),\n",
        "        'True positive rate': tpr.astype('float64'),\n",
        "        'Class': f\"Micro-average (area: {fix_float(auc(fpr, tpr), 3, 3)})\"\n",
        "    })]).reset_index(drop=True)\n",
        "\n",
        "  def get_mean_tpr():\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for class_val in classes:\n",
        "      mean_tpr += np.interp(\n",
        "          all_fpr,\n",
        "          df['False positive rate'],\n",
        "          df['True positive rate']\n",
        "      )\n",
        "    mean_tpr /= classes.size\n",
        "    return mean_tpr\n",
        "\n",
        "  df = pd.concat([df, pd.DataFrame({\n",
        "      'False positive rate': all_fpr, \n",
        "      'True positive rate': get_mean_tpr(), \n",
        "      'Class': f\"Macro-average (area: {fix_float(auc(all_fpr, get_mean_tpr()), 3, 3)})\"\n",
        "  })]).reset_index(drop=True)\n",
        "\n",
        "  def plot():\n",
        "    sns.relplot(\n",
        "      data=df,\n",
        "      x='False positive rate',\n",
        "      y='True positive rate',\n",
        "      kind='line',\n",
        "      hue='Class',\n",
        "      style='Class',\n",
        "      palette=sns.color_palette(\n",
        "          palette='Pastel2',\n",
        "          n_colors=df['Class'].unique().size\n",
        "      ),\n",
        "      aspect=1.2\n",
        "  )\n",
        "  plot()\n",
        "\n",
        "\n",
        "\n",
        "get_roc_curve(train_y, p)\n"
      ],
      "metadata": {
        "id": "ZA8jSctRsBx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ensemble",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}